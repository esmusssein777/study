# Docker 容器

[toc]

## 发展历史

让我们将历史拨回到 1979 年，贝尔实验室正在开发 unix v7（Version 7 Unix） 操作系统。



项目进行到了最后的开发和测试阶段，即便是对于这些天才般的程序员来说，系统级别的软件构建和测试仍然是一个繁复且无比棘手的难题。



你可以想象：当一次测试开始，源代码编译和安装完成之后，整个测试环境就被“污染”了。要想进行下一次测试，就需要重新搭建，并且再编译安装测试。那有没有办法来避免重复的环境搭建，快速的销毁和重建所需的基础设施环境呢？



于是，一个叫做 chroot（Change Root） 的系统调用就此诞生了！

chroot 会重定向进程及其子进程的根目录到操作系统上的一个新位置，使得该进程的文件“视图”与外面的“世界”完全隔离，它不能够对这个指定根目录之外的文件进行访问动作，不能读取，也不能更改它的内容。



在 2013 年左右的时候，虚拟机和云计算技术已经普及开来，那时主流用户的普遍用法，就是租一批 AWS 或者 OpenStack 的虚拟机，然后像以前管理物理服务器那样，用脚本或者手工的方式在这些机器上部署应用。当然，这个部署过程难免会碰到云端虚拟机和本地环境不一致的问题，所以当时的云计算服务， 比的就是谁能更好地模拟本地服务器环境，能带来更好的“上云”体验。

而 PaaS 开源项目Cloud Foundry 就是最开始火起来的项目，Cloud Foundry 提供了一套应用的打包和分发机制，运维人员在一些机器上部署一个 Cloud Foundry 项目，开发可以把可执行文件和启动脚本打进一个压缩包里面，使用 `cf push` 命令上传到云上 Cloud Foundry 存储中，接着，Cloud Foundry 会通过调度器选择一个可以运行这个应用的虚拟机，然后通知这个机器上的 Agent 把应用压缩包下载下来启动。

> https://github.com/cloudfoundry/cli

当需要在一个虚拟机上启动很多个来自不同用户的应用，Cloud Foundry 会调用操作系统的 Cgroups 和 Namespace 机制为每一个应用单独创建一个称作“沙盒”的隔离环境，而这些 Cloud Foundry 用来运行应用的隔离环境，或者说“沙盒”，就是所谓的“容器”。

这么一看 Cloud Foundry 似乎非常的完美，和 Docker 现在的功能并没有什么区别，但现实真的是这样吗？

Cloud Foundry 之所以能够帮助用户大规模部署应用到集群里，是因为它提供了一套应用打包的功能。可偏偏就是这个打包功能经常有问题，用户就必须为每种语言、每种框架，甚至每个版本的应用维护一个打好的包。这个打包过程，没有任何章法可循，更麻烦的是，明明在本地运行得好好的应用，却需要做很多修改和配置工作才能在 PaaS 里运行起来。而这些修改和配置， 并没有什么经验可以借鉴，基本上得靠不断试错才能搞定。

最后结局就是，“cf push”确实是能一键部署了，但是为了实现这个一键部署，用户为每个应用打包的工作非常的痛苦。



而 Docker 解决的，恰恰就是打包这个根本性的问题。 所谓 Docker 镜像，其实就是一个压缩包。但是这个压缩包里的内容，比 Cloud Foundry 的应用可执行文件 + 启停脚本的组合就要丰富多了。实际上，大多数 Docker 镜像是直接由**一个完整操作系统的所有文件和目录**构成的，所以这 个压缩包里的内容跟你本地开发和测试环境用的操作系统是完全一样的。

假设你的应用在本地运行时，能看见的环境是 CentOS 7.2 操作系统的所有文件和目录，那么只要用 CentOS 7.2 的 ISO 做一个压缩包，再把你的应用可执行文件也压缩进去，那么无论在哪里解压这个压缩包，都可以得到与你本地测试时一样的环境。当然，你的应用也在里面!

这就是 Docker 镜像最厉害的地方:只要有这个压缩包在手，你就可以使用某种技术创建一 个“沙盒”，在“沙盒”中解压这个压缩包，然后就可以运行你的程序了。

Docker 赋予了你一种极其宝贵的能力:本地环境和云端环境的高度一致! 这，正是 Docker 镜像的精髓。

不过，Docker 项目固然解决了应用打包的难题，但正如前面所介绍的那样，它并不能代替 Cloud Foundry 完成大规模部署应用的职责。而这正是 k8s 崛起的原因。

## 容器基础

容器其实是一种沙盒 技术。顾名思义，沙盒就是能够像一个集装箱一样，把你的应用“装”起来的技术。这样，应用与应用之间，就因为有了边界而不至于相互干扰;而被装进集装箱的应用，也可以被方便地搬来搬去。

那么怎么样才能把应用给隔离开来呢？Docker 使用 Namespace 技术来修改进程视图，使用 Cgroups 技术来制造约束，给程序创造一个边界。

### Namespace

本来，每当我们在宿主机上运行了一个 /bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID=100。这个编号就是进程的唯一标识。而现在，我们通过 Docker 给程序的进程空间进行修改，使得进程只能看到重新计算过的进程编号，比如 PID = 1 ，可实际上，它们在宿主机操作系统中，还是原来的 100 号进程。

这种技术，就是 Linux 里面的 Namespace 机制。而 Namespace 的使用方式也非常有意思: 它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建线程的系统调用是 clone()。

```
int pid = clone(main_function, stack_size, SIGCHLD, NULL);
```

这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。

而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如:

```
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL);
```

这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。在宿主机真实的进程空间里，这个进程 的 PID 还是真实的数值，比如 100。

当然，我们还可以多次执行上面的 clone() 调用，这样就会创建多个 PID Namespace，而每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况。

而除了我们刚刚用到的 PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、 Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。

比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息; Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。

### Cgroups

我们通过 Namespace 给进程修改视图后，的确它看不到其它的进程情况了，但是在宿主机上，它和其它的进程之间依旧是平等的关系，它给使用到的资源随时被其它的进程占用，也可能它自己就占用了 100% 的宿主机资源，这些情况显然都不是正常的行为。

而Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够 使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。

在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织 在操作系统的 /sys/fs/cgroup 路径下。在 Ubuntu 16.04 机器里，我可以用 mount 指令把它们展示出来，这条命令是:

```
$ mount -t cgroup
cpuset on /sys/fs/cgroup/cpuset type cgroup ...
cpu on /sys/fs/cgroup/cpu type cgroup ...
cpuacct on /sys/fs/cgroup/cpuacct type cgroup ...
blkio on /sys/fs/cgroup/blkio type cgroup ...
memory on /sys/fs/cgroup/memory type cgroup ...
```

在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是:

```
ls /sys/fs/cgroup/cpu
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.sharesnotify_on_release cgroup.procs cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat tasks
```

cfs_period 和 cfs_quota 这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只 能被分配到总量为 cfs_quota 的 CPU 时间。

你需要在对应的子系统下面创建一个目录，比如，我们现在进入 /sys/fs/cgroup/cpu 目录下:

```
root@ubuntu:/sys/fs/cgroup/cpu$ mkdir container
root@ubuntu:/sys/fs/cgroup/cpu$ ls container/
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release
cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks
```

这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。

现在，我们在后台执行这样一条脚本:

```
$ while : ; do : ; done &
[1] 226
```

显然，它执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号(PID)是 226。可以用 top 指令来确认一下 CPU 情况。

而此时，我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制(即:-1)，CPU period 则是默认的 100 ms(100000 us):

```
$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
-1
$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us
100000
```

接下来，我们可以通过修改这些文件的内容来设置限制。

比如，向 container 组里的 cfs_quota 文件写入 20 ms(20000 us):

```
echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
```

结合前面的介绍，你应该能明白这个操作的含义，它意味着在每 100 ms 的时间里，被该控制组 限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。

接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该 进程生效了:

```
$ echo 226 > /sys/fs/cgroup/cpu/container/tasks
```

我们可以用 top 指令查看一下计算机的 CPU 使用率立刻降到了 20%(%Cpu0 : 20.3 us)

除 CPU 子系统外，Cgroups 的每一项子系统都有其独有的资源限制能力，比如:

* blkio，为块设备设定I/O 限制，一般用于磁盘等设备; 

* cpuset，为进程分配单独的 CPU 核和对应的内存节点; 

* memory，为进程设定内存使用的限制。

Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组 资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组(即创建一个新目录)，然后在启动容器进程之后，把这个进程 的 PID 填写到对应控制组的 tasks 文件中就可以了。

跟 Namespace 的情况类似，Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。

众所周知，Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。

但是，你如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。造成这个问题的原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即:/proc 文件系统不了解 Cgroups 限制的存在。

### rootfs

如何做到进到容器中默认的根路径是宿主机中的某一个文件呢？而不是和宿主机根文件路径呢？

在 Linux 操作系统里，有一个名为 chroot 的命令可以帮助你在 shell 中方便地完成这个工作。 顾名思义，它的作用就是帮你“change root file system”，即改变进程的根目录到你指定的位置。`chroot $Home/test /bin/bash`

对于被 chroot 的进程来说，它不会感受到自己的根目录已经被修改成宿主机的某一个目录，它执行 `ls /` 会看到的是 `$Home/test`

实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。

当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 "ls /" 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。

而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作:rootfs(根文件系统)。

另外，需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。

正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性:一致性。

由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。

有了容器镜像“打包操作系统”的能力，这个最基础的依赖环境也终于变成了应用沙盒的一部分。这就赋予了容器所谓的一致性:无论在本地、云端，还是在一台任何地方的机器上，用户只需要解压打包好的容器镜像，那么这个应用运行所需要的完整的执行环境就被重现出来了。这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。

### Docker image

难道我每开发一个应用，或者升级一下现有的应用，都要重复制作一次 rootfs 吗?

既然这些修改都基于一个旧的 rootfs，我们能不能以增量的方式去做这些修改呢?这样 做的好处是，所有人都只需要维护相对于 base rootfs 修改的增量内容，而不是每次修改都制造 一个“fork”。

> Docker 在镜像的设计中，引入了层(layer)的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。

Union File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载(union mount)到同一个目录下。比如，我现在有两个目录 A 和 B，它们分别有两个文件，进行 mount aufs 到 C 目录:

```
├── A
│  ├── a
│  └── x
└── B
	├── b 
	└── x
```

```
mkdir C
mount -t aufs -o dirs=./A:./B none .C
```

```
├── a
├── b
└── x
```

在 Docker 中，正是使用 Union FIle System 来制造镜像。来看下 Docker 的镜像：

```
$ docker image inspect ubuntu:latest
...
     "RootFS": {
      "Type": "layers",
      "Layers": [
        "sha256:f49017d4d5ce9c0f544c...",
        "sha256:8f2b771487e9d6354080...",
        "sha256:ccd4d61916aaa2159429...",
        "sha256:c01d74f99de40e097c73...",
        "sha256:268a067217b5fe78e000..."
] }
```

可以看到，这个 Ubuntu 镜像，实际上由五个层组成。这五个层就是五个增量 rootfs，每一层 都是 Ubuntu 操作系统文件与目录的一部分;而在使用镜像时，Docker 会把这些增量联合挂载 在一个统一的挂载点上(等价于前面例子里的“/C”目录)。

这个容器的 rootfs 由如下图所示的三部分组成:

![image-20210417154610097](/Users/guangzheng.li/Library/Application Support/typora-user-images/image-20210417154610097.png)

#### 第一部分，只读层。

它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。可以看到，它 们的挂载方式都是只读的(ro+wh)，即 readonly+whiteout

只读层以增量的方式分别包含了 Ubuntu 操作系统的一部分。

#### 第二部分，可读写层。

它是这个容器的 rootfs 最上面的一层(6e3be5d2ecccae7cc)，它的挂载方式为:rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。

如何理解呢？对于新增好理解，如果是更新和删除呢？

为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文 件“遮挡”起来。比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了 一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文 件“遮挡”起来，“消失”了。这个功能，就是“ro+wh”的挂载方式，即只读 +whiteout 的含义。

所以，最上面这个可读写层的作用，就是专门用来存放你修改 rootfs 后产生的增量，无论是 增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用;而与此同时，原先的只读层里的内容则不会有任何变化。这就是增量 rootfs 的好处。

#### 第三部分，Init 层。

它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一 个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。

需要这样一层的原因是，这些文件本来属于只读的 Ubuntu 镜像的一部分，但是用户往往需要在启动容器时写入一些指定的值比如 hostname，所以就需要在可读写层对它们进行修改。

可是，这些修改往往只对当前的容器有效，我们并不希望执行 docker commit 时，把这些信息连同可读写层一起提交掉。

所以，Docker 做法是，在修改了这些文件之后，以一个单独的层挂载了出来。而用户执行 docker commit 只会提交可读写层，所以是不包含这些内容的。

最终，这 7 个层都被联合挂载到 /var/lib/docker/aufs/mnt 目录下，表现为一个完整的 Ubuntu 操作系统供容器使用。



于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多;而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。



## 容器总结

![iPtcLD](https://cdn.jsdelivr.net/gh/guangzhengli/ImgURL@master/uPic/iPtcLD.png)

上图是虚拟机和容器的对比图，为什么 Docker 项目比虚拟机更受欢迎？

这是因为，使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就 不可避免地带来了额外的资源消耗和占用。

根据实验，一个运行着 CentOS 的 KVM 虚拟机启动后，在不做优化的情况下，虚拟机自己就 需要占用 100~200 MB 内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用 就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。

而相比之下，容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的;而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。

所以说，“敏捷”和“高性能”是容器相较于虚拟机最大的优势，也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因。

不过，有利就有弊，基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处， 其中最主要的问题就是:隔离得不彻底。

首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。

尽管你可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。

而相比之下，拥有硬件虚拟化技术和独立 Guest OS 的虚拟机就要方便得多了。最极端的例子 是，Microsoft 的云计算平台 Azure，实际上就是运行在 Windows 服务器集群上的，但这并不妨碍你在它上面创建各种 Linux 虚拟机出来。

其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是: 时间。这就意味着，当某个容器修改了某些参数时，其它的容器也会受到影响，这也意味着安全问题和系统调用问题。



参考：https://www.youtube.com/watch?v=8fi7uSYlOdc