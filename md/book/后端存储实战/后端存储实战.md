# 电商存储

[toc]

## 确保数据准确性

因为网络、服务器等等这些不确定的因素，重试请求是普遍存在并且不可避免的。具有幂等性的服务可以完美地克服重试导致的数据错误。

对于创建订单服务来说，可以通过预先生成订单号，然后利用数据库中订单号的唯一约束这 个特性，避免重复写入订单，实现创建订单服务的幂等性。对于更新订单服务，可以通过一 个版本号机制，每次更新数据前校验版本号，更新数据同时自增版本号，这样的方式，来解 决 ABA 问题，确保更新订单服务的幂等性。

通过这样两种幂等的实现方法，就可以保证，无论请求是不是重复，订单表中的数据都是正确的。

## 数据库如何设计

考虑两个问题：

1. 高并发问题
2. 商品数据规模问题

需要存储的信息有：

1. 基本信息
2. 商品参数
3. 商品介绍
4. 图片视频

### 基本信息

我们先来分析商品的基本信息，它包括商品的主副标题、价格、颜色等一些商品最基本、主要的属性。这些属性都是固定的，不太可能会因为需求或者不同的商品而变化，而且，这部分数据也不会太大。所以，还是建议你在数据库中建一张表来保存商品的基本信息。

然后，还需要在数据库前面，加一个缓存，帮助数据抵挡绝大部分的读请求。这个缓存，你可以使用 Redis，也可以用 Memcached，这两种存储系统都是基于内存的 KV 存储，更新商品信息的时候，在更新数据库的同时，也要把缓存中的数据给删除掉。不然就有可能出现这种情况:数据库中的数据变了，而缓存中的数据没变，商详页上看到的还是旧数据。

这种缓存更新的策略，称为 Cache Aside，是最简单实用的一种缓存更新策略，适用范围也最广泛。如果你要缓存数据，没有什么特殊的情况，首先就应该考虑使用这个策略。

除了 Cache Aside 以外，还有 Read/Write Through、Write Behind 等几种策略，分别 适用于不同的情况。

### 使用 **MongoDB** 保存商品参数

我们再来分析商品参数，参数就是商品的特征。比如说，电脑的内存大小、手机的屏幕尺寸、酒的度数、口红的色号等等。和商品的基本属性一样，都是结构化的数据。但麻烦的是，不同类型的商品，它的参数是完全不一样的。如果我们设计一个商品参数表，那这个表的字段就会太多了，并且每增加一个品类的商品，这个表就要加字段，这个方案行不通

MongoDB 是一个面向文档存储的 NoSQL 数据库，在 MongoDB 中，表、行、列对应的 概念分别是:collection、document、field。对于商品参数信息，数据量大、数据结构不统一，这些 MongoDB 都可以很好的满足。

### 使用对象存储保存图片和视频

对象存储可以简单理解为一个无限容量的大文件 KV 存储，它的存储单位是对象，其实就是 文件，可以是一张图片，一个视频，也可以是其他任何文件。每个对象都有一个唯一的 key，利用这个 key 就可以随时访问对应的对象。基本的功能就是写入、访问和删除对象。

### 将商品介绍静态化

商品介绍在商详页中占得比重是最大的，包含了大量的带格式文字、图片和视频。其中图片和视频自然要存放在对象存储里面，商品介绍的文本，一般都是随着商详页一起静态化，保存在 HTML 文件中。商详页的绝大部分内容都是商品介绍，它是不怎么变的。那不如就把这个页面事先生成好， 保存成一个静态的 HTML，访问商详页的时候，直接返回这个 HTML。这就是静态化。

## 购物车

购物车系统的主要功能包括:加购、购物车列表页和结算下单。核心的实体就只有一个“购 物车”实体，它至少要包括:SKUID、数量、加购时间和勾选状态这几个属性。

在给购物车设计存储时，为了确保购物车内的数据在多端保持一致，以及用户登录前后购物车内商品能无缝衔接，除了每个用户的“用户购物车”之外还要实现一个“暂存购物车”保存用户未登录时加购的商品，并在用户登录后自动合并“暂存购物车”和“用户购物车”。

暂存购物车存储在客户端浏览器或者 App 中，可以选择存放到 Cookie 或者 LocalStorage 中。用户购物车保存在服务端，可以选择使用 Redis 或者是 MySQL 存储，使用 Redis 存储会有更高的性能，可以支撑更多的并发请求，使用 MySQL 是更常规通用的方式，便于 应对变化，系统的扩展性更好。

## 余额

在设计系统的存储时，原则上不应该存储冗余数据，一是浪费存储空间，二是让这些冗余数据保持一致是一件非常麻烦的事儿。但有些场景下存储冗余数据是必要的，比如用户账户的余额这个数据。

这个数据在交易过程中会被非常频繁地用到，总不能每次交易之前，先通过所有历史交易记 录计算一下当前账户的余额，这样做速度太慢了，性能满足不了交易的需求。所以账户系统保存了每个用户的账户余额，这实际上是一种用存储空间换计算时间的设计。

如果说只是满足功能需求，账户系统只记录余额，每次交易的时候更新账户余额就够了。但 是这样做有一个问题，如果账户余额被篡改，是没有办法追查的，所以在记录余额的同时， 还需要记录每一笔交易记录，也就是账户的流水。流水的数据模型至少需要包含:流水 ID、交易金额、交易时间戳以及交易双方的系统、账户、交易单号等信息。

## 分布式事务

如果在一个系统内我们要保证事务性，只用选择在系统和MySQL的事务即可，在多个服务间保证数据的一致性，就要使用分布式事务。

分布式事务的解决方案有很多，比如:2PC、3PC、TCC、Saga 和本地消息表等等。这些 方法，它的强项和弱项都不一样，适用的场景也不一样，所以最好这些分布式事务你都能够 掌握，这样才能在面临实际问题的时候选择合适的方法。这里面，2PC 和本地消息表这两 种分布式事务的解决方案，比较贴近于我们日常开发的业务系统。

### 2PC:订单与优惠券的数据一致性问题

2PC 是怎么解决这个问题的。2PC 引入了一个事务协调者的角色，来协调 订单系统和促销系统，协调者对客户端提供一个完整的“使用优惠券下单”的服务，在这个 服务的内部，协调者再分别调用订单和促销的相应服务。

所谓的二阶段指的是准备阶段和提交阶段。在准备阶段，协调者分别给订单系统和促销系统发送“准备”命令，订单和促销系统收到准备命令之后，开始执行准备操作。准备阶段都需要做哪些事儿呢?你可以理解为，除了提交数据库事务以外的所有工作，都要在准备阶段完成。比如说订单系统在准备阶段需要完成:

1. 在订单库开启一个数据库事务;

2. 在“订单优惠券表”写入这条订单的优惠券记录; 
3. 在“订单表”中写入订单数据。

注意，到这里我们没有提交订单数据库的事务，最后给事务协调者返回“准备成功”。类似的，促销服务在准备阶段，需要在促销库开启一个数据库事务，更新优惠券状态，但是暂时不要提交这个数据库事务，给协调者返回“准备成功”。协调者在收到两个系统“准备成功”的响应之后，开始进入第二阶段。

等两个系统都准备好了之后，进入提交阶段。提交阶段就比较简单了，协调者再给这两个系统发送“提交”命令，每个系统提交自己的数据库事务，然后给协调者返回“提交成功”响应，协调者收到所有响应之后，给客户端返回成功响应，整个分布式事务就结束了。

异常情况下怎么办?

我们还是分两个阶段来说明。在准备阶段，如果任何一步出现错误或者是超时，协调者就会给两个系统发送“回滚事务”请求。每个系统在收到请求之后，回滚自己的数据库事务，分布式事务执行失败，两个系统的数据库事务都回滚了，相关的所有数据回滚到分布式事务执行之前的状态，就像这个分布式事务没有执行一样。

如果准备阶段成功，进入提交阶段，这个时候就“只有华山一条路”，整个分布式事务只能 成功，不能失败。

如果发生网络传输失败的情况，需要反复重试，直到提交成功为止。如果这个阶段发生宕 机，包括两个数据库宕机或者订单服务、促销服务所在的节点宕机，还是有可能出现订单库 完成了提交，但促销库因为宕机自动回滚，导致数据不一致的情况。但是，因为提交的过程 非常简单，执行很快，出现这种情况的概率非常小，所以，从实用的角度来说，2PC 这种 分布式事务的方法，实际的数据一致性还是非常好的。

2PC 是一种强一致的设计，它可以保证原子性和隔离性。只要 2PC 事务完成，订单库和促 销库中的数据一定是一致的状态，也就是我们总说的，要么都成功，要么都失败。

所以 2PC 比较适合那些对数据一致性要求比较高的场景，比如我们这个订单优惠券的场 景，如果一致性保证不好，有可能会被黑产利用，一张优惠券反复使用，那样我们的损失就 大了。

2PC 也有很明显的缺陷，整个事务的执行过程需要阻塞服务端的线程和数据库的会话，所 以，2PC 在并发场景下的性能不会很高。并且，协调者是一个单点，一旦过程中协调者宕 机，就会导致订单库或者促销库的事务会话一直卡在等待提交阶段，直到事务超时自动回 滚。

卡住的这段时间内，数据库有可能会锁住一些数据，服务中会卡住一个数据库连接和线程，

这些都会造成系统性能严重下降，甚至整个服务被卡住。

所以，只有在需要强一致、并且并发量不大的场景下，才考虑使用 2PC。

## 本地消息表:订单与购物车的数据一致性问题

2PC 它的适用场景其实是很窄的，更多的情况下，只要保证数据最终一致就可以了。比如说，在购物流程中，用户在购物车界面选好商品后，点击“去结算”按钮进入订单页面创建 一个新订单。这个过程我们的系统其实做了两件事儿。

 第一，订单系统需要创建一个新订单，订单关联的商品就是购物车中选择的那些商品。

 第二，创建订单成功后，购物车系统需要把订单中的这些商品从购物车里删掉。

这也是一个分布式事务问题，创建订单和清空购物车这两个数据更新操作需要保证，要么都成功，要么都失败。但是，清空购物车这个操作，它对一致性要求就没有扣减优惠券那么高，订单创建成功后，晚几秒钟再清空购物车，完全是可以接受的。只要保证经过一个小的延迟时间后，最终订单数据和购物车数据保持一致就可以了。

本地消息表非常适合解决这种分布式最终一致性的问题。我们一起来看一下，如何使用本地消息表来解决订单与购物车的数据一致性问题。

本地消息表的实现思路是这样的，订单服务在收到下单请求后，正常使用订单库的事务去更新订单的数据，并且，在执行这个数据库事务过程中，在本地记录一条消息。这个消息就是一个日志，内容就是“清空购物车”这个操作。因为这个日志是记录在本地的，这里面没有分布式的问题，那这就是一个普通的单机事务，那我们就可以让订单库的事务，来保证记录本地消息和订单库的一致性。完成这一步之后，就可以给客户端返回成功响应了。

然后，我们再用一个异步的服务，去读取刚刚记录的清空购物车的本地消息，调用购物车系统的服务清空购物车。购物车清空之后，把本地消息的状态更新成已完成就可以了。异步清空购物车这个过程中，如果操作失败了，可以通过重试来解决。最终，可以保证订单系统和购物车系统它们的数据是一致的。

这里面，本地消息表，你可以选择存在订单库中，也可以用文件的形式，保存在订单服务所在服务器的本地磁盘中，两种方式都是可以的，相对来说，放在订单库中更简单一些。

消息队列 RocketMQ 提供一种事务消息的功能，其实就是本地消息表思想的一个实现。使用事务消息可以达到和本地消息表一样的最终一致性，相比我们自己来实现本地消息表，使用起来更加简单，你也可以考虑使用。

## 如何安全的做数据备份和同步

### 备份

数据备份时，使用低频度的全量备份配合 Binlog 增量备份是一种常用而且非常实用的方法，使用这种备份方法，我们可以把数据库的数据精确地恢复到历史上任意一个时刻，不仅能解决数据损坏的问题，也不用怕误操作、删库跑路这些事儿了。特别要注意的是，让备份数据尽量地远离数据库。

### 同步

复制数据的时候，只要基于一个快照，按照顺序执行快照之后的所有操作日志，就可以得到一个完全一样的状态。在从节点持续地从主节点上复制操作日志并执行，就可以让从节点上的状态数据和主节点保持同步。

主从同步做数据复制时，一般可以采用几种复制策略。性能最好的方法是异步复制，主节点上先记录操作日志，再更新状态数据，然后异步把操作日志复制到所有从节点上，并在从节点执行操作日志，得到和主节点相同的状态数据。

异步复制的劣势是，可能存在主从延迟，如果主节点宕机，可能会丢数据。另外一种常用的 策略是半同步复制，主节点等待操作日志最少成功复制到 N 个从节点上之后，再更新状 态，这种方式在性能、高可用和数据可靠性几个方面都比较平衡，很多分布式存储系统默认 采用的都是这种方式。

#### MySQL 同步到 Redis 或其它

可以使用 Canal ，Canal 模拟 MySQL 主从复制的交互协议，把自己伪装成一个 MySQL 的从节点，向 MySQL 主节点发送 dump 请求，MySQL 收到请求后，就会开始推送 Binlog 给 Canal， Canal 解析 Binlog 字节流之后，转换为便于读取的结构化数据，供下游程序订阅使用。下游可以是Redis缓存，也可以是ElasticSearch之类的其它数据库。

为了能够支撑下游众多的数据库，从 Canal 出来的 Binlog 数据肯定不能直接去写下游那么多数据库，一是写不过来，二是对于每个下游数据库，它可能还有一些数据转换和过滤的工 作要做。所以需要增加一个 MQ 来解耦上下游。

Canal 从 MySQL 收到 Binlog 并解析成结构化数据之后，直接写入到 MQ 的一个订单 Binlog 主题中，然后每一个需要同步订单数据的业务方，都去订阅这个 MQ 中的订单 Binlog 主题，消费解析后的 Binlog 数据。在每个消费者自己的同步程序中，它既可以直接 入库，也可以做一些数据转换、过滤或者计算之后再入库，这样就比较灵活了。

Canal 和 MQ 这两个环节，由于没什么业务逻辑，性能都非常好。所以，一般容易成为性能瓶颈的 就是消费 MQ 的同步程序，因为这些同步程序里面一般都会有一些业务逻辑，而且如果下游的数据库写性能跟不上，表象也是这个同步程序处理性能上不来，消息积压在 MQ 里面。

那我们能不能多加一些同步程序的实例数，或者增加线程数，通过增加并发来提升处理能力呢?这个地方的并发数，还真不是随便说扩容就可以就扩容的，我来跟你讲一下为什么。

我们知道，MySQL 主从同步 Binlog，是一个单线程的同步过程。为什么是单线程?原因很简单，在从库执行 Binlog 的时候，必须按顺序执行，才能保证数据和主库是一样的。为了确保数据一致性，Binlog 的顺序很重要，是绝对不能乱序的。 严格来说，对于每一个 MySQL 实例，整个处理链条都必须是单线程串行执行，MQ 的主题也必须设置为只有 1 个 分区(队列)，这样才能保证数据同步过程中的 Binlog 是严格有序的，写到目标数据库的数据才能是正确的。

我们只要保证每个订单的更新操作日志的顺序别乱就可以了。这种一致性要求称 为因果一致性(Causal Consistency)。首先根据下游同步程序的消费能力，计算出需要多少并发;然后设置 MQ 中主题的分区 (队列)数量和并发数一致。因为 MQ 是可以保证同一分区内，消息是不会乱序的，所以 我们需要把具有因果关系的 Binlog 都放到相同的分区中去，就可以保证同步数据的因果一致性。对应到订单库就是，相同订单号的 Binlog 必须发到同一个分区上。

## 缓存

最原始的方法，在查询订单数据的时候，先去缓存中查询，如果命中缓存那就直接返回订单数据。如果没有命中，那就去数据库中查询，得到查询结果之后把订单数据写入缓存，然后返回。在更新订单数据的时候，先去更新数据库中的订单表，如果更新成功，再去更新缓存中的数据。

这其实是一种经典的缓存更新策略: Read/Write Through。这样使用缓存的方式有没有问 题?绝大多数情况下可能都没问题。但是，在并发的情况下，有一定的概率会出现“脏数 据”问题，缓存中的数据可能会被错误地更新成了旧数据。

比如，对同一条订单记录，同时产生了一个读请求和一个写请求，这两个请求被分配到两个不同的线程并行执行，读线程尝试读缓存没命中，去数据库读到了订单数据，这时候可能另外一个读线程抢先更新了缓存，在处理写请求的线程中，先后更新了数据和缓存，然后，拿着订单旧数据的第一个读线程又把缓存更新成了旧数据。



Cache Aside 模式可以很好地解决这个问题，在大多数 情况下是使用缓存的最佳方式。

Cache Aside 模式和上面的 Read/Write Through 模式非常像，它们处理读请求的逻辑是 完全一样的，唯一的一个小差别就是，Cache Aside 模式在更新数据的时候，并不去尝试 更新缓存，而是去删除缓存。

## 海量数据优化

### 存档历史订单数据提升查询性能

当单表的订单数据太多，多到影响性能的时候，首选的方案是，归档历史订单。

所谓归档，其实也是一种拆分数据的策略。简单地说，就是把大量的历史订单移到另外一张历史订单表中。为什么这么做呢?因为像订单这类具有时间属性的数据，都存在热尾效应。大多数情况下访问的都是最近的数据，但订单表里面大量的数据都是不怎么常用的老数据。

因为新数据只占数据总量中很少的一部分，所以把新老数据分开之后，新数据的数据量就会少很多，查询速度也就会快很多。老数据虽然和之前比起来没少多少，查询速度提升不明显，但是，因为老数据访很少会被访问到，所以慢一点儿也问题不大。

这样拆分的另外一个好处是，拆分订单时，需要改动的代码非常少。大部分对订单表的操作 都是在订单完成之前，这些业务逻辑都是完全不用修改的。即使像退货退款这类订单完成后 的操作，也是有时限的，那这些业务逻辑也不需要修改，原来该怎么操作订单表还怎么操作。

归档历史订单，大致的流程是这样的:

1. 首先我们需要创建一个和订单表结构一模一样的历史订单表;

2. 然后，把订单表中的历史订单数据分批查出来，插入到历史订单表中去。这个过程你怎么实现都可以，用存储过程、写个脚本或者写个导数据的小程序都行，用你最熟悉的方法就行。如果你的数据库已经做了主从分离，那最好是去从库查询订单，再写到主库的历史订单表中去，这样对主库的压力会小一点儿。

3. 现在，订单表和历史订单表都有历史订单数据，先不要着急去删除订单表中的数据，你应该测试和上线支持历史订单表的新版本代码。因为两个表都有历史订单，所以现在这个数据库可以支持新旧两个版本的代码，如果新版本的代码有 Bug，你还可以立刻回滚到旧版本，不至于影响线上业务。

4. 等新版本代码上线并验证无误之后，就可以删除订单表中的历史订单数据了。

5. 最后，还需要上线一个迁移数据的程序或者脚本，定期把过期的订单从订单表搬到历史 订单表中去。

这里面还有一个很重要的细节问题:如何从订单表中删除已经迁走的历史订单数据?我们直 接执行一个删除历史订单的 SQL 行不行?大概率你会遇到错误，提示删除失败，因为需要删除的数据量太大了，所以需要分批删除。 比如说我们每批删除 1000 条记录

### 分库分表

我们分库分表的目的是为了解决两个问题:

第一，是数据量太大查询慢的问题。这里面我们讲的“查询”其实主要是事务中的查询和更 新操作，因为只读的查询可以通过缓存和主从分离来解决，解决查询慢，只要减少每次查询的数据总量就可以了，也就是说，分表就可以解决问题。

第二，是为了应对高并发的问题。应对高并发的思想我们之前也说过，一个数据库实例撑不住，就把并发请求分散到多个实例中去，所以，解决高并发的问题是需要分库的。

简单地说，数据量大，就分表; 并发高，就分库。

#### 如何选择 **Sharding Key**?

分库分表还有一个重要的问题是，选择一个合适的列或者说是属性，作为分表的依据，这个 属性一般称为 Sharding Key。归档历史订单的方法，它的 Sharding Key 就是订单完成时间。每次查询的时候，查询条件中必须带上这个时间，我们的程序就知道，三个月以前的数据查订单历史表，三个月内的数据查订单表，这就是一个简单的按照时间范围来分片的算法。

选择合适 Sharding Key 和分片算法非常重要，直接影响了分库分表的效果。我们首先来说 如何选择 Sharding Key 的问题。

选择这个 Sharding Key 最重要的参考因素是，我们的业务是如何访问数据的。

比如我们把订单 ID 作为 Sharding Key 来拆分订单表，那拆分之后，如果我们按照订单 ID 来查订单，就需要先根据订单 ID 和分片算法计算出，我要查的这个订单它在哪个分片上， 也就是哪个库哪张表中，然后再去那个分片执行查询就可以了。

但是，当我打开“我的订单”这个页面的时候，它的查询条件是用户 ID，这里没有订单 ID，那就没法知道我们要查的订单在哪个分片上，就没法查了。当然你要强行查的话，那 就只能把所有分片都查一遍，再合并查询结果，这个就很麻烦，而且性能很差，还不能分页。

那要是把用户 ID 作为 Sharding Key 呢?也会面临同样的问题，使用订单 ID 作为查询条 件来查订单的时候，就没办法找到订单在哪个分片了。这个问题的解决办法是，在生成订单 ID 的时候，把用户 ID 的后几位作为订单 ID 的一部分，比如说，可以规定，18 位订单号 中，第 10-14 位是用户 ID 的后四位，这样按订单 ID 查询的时候，就可以根据订单 ID 中 的用户 ID 找到分片。

一旦做了分库分表，就会极大地限制数据库的查询能力，之前很简单的查询，分库分表之后，可能就没法实现了。分库分表一定是，数据量和并发大到所有招数都不好使了，我们才拿出来的最后一招。

## Redis 分布式

### Redis Cluster 如何解决数据量大、高可用和高并发问题?

Redis 从 3.0 版本开始，提供了官方的集群支持，也就是 Redis Cluser。Redis Cluster 相 比于单个节点的 Redis，能保存更多的数据，支持更多的并发，并且可以做到高可用，在单个节点故障的情况下，继续提供服务。Redis Cluster 是如何来分片的呢?它引入了一个“槽(Slot)”的概念，这个槽就是哈希表中的哈希槽，槽是 Redis 分片的基本单位，每个槽里面包含一些 Key。每个集群的槽数是固定的 16384(16 * 1024) 个，每个 Key 落在哪个槽中也是固定的，计算方法是:

这个算法很简单，先计算 Key 的 CRC 值，然后把这个 CRC 之后的 Key 值直接除以 16384，余数就是 Key 所在的槽。这个算法就是我们上节课讲过的哈希分片算法。

客户端可以连接集群的任意一个节点来访问集群的数据，当客户端请求一个 Key 的时候， 被请求的那个 Redis 实例先通过上面的公式，计算出这个 Key 在哪个槽中，然后再查询槽 和节点的映射关系，找到数据所在的真正节点，如果这个节点正好是自己，那就直接执行命 令返回结果。如果数据不在当前这个节点上，那就给客户端返回一个重定向的命令，告诉应该去连哪个节点上请求这个 Key 的数据。然后客户端会再连接正确的节点来访问。

分片可以解决 Redis 保存海量数据的问题，并且客观上提升了 Redis 的并发能力和查询性 能。但是并不能解决高可用的问题，每个节点都保存了整个集群数据的一个子集，任何一个 节点宕机，都会导致这个宕机节点上的那部分数据无法访问。

### Redis Cluster 是怎么解决高可用问题的?

参见上面我们讲到的方法:增加从节点，做主从复制。Redis Cluster 支持给每个分片增加 一个或多个从节点，每个从节点在连接到主节点上之后，会先给主节点发送一个 SYNC 命 令，请求一次全量复制，也就是把主节点上全部的数据都复制到从节点上。全量复制完成之 后，进入同步阶段，主节点会把刚刚全量复制期间收到的命令，以及后续收到的命令持续地转发给从节点。

### 为什么 **Redis Cluster** 不适合超大规模集群?

Redis Cluster 的优点是易于使用。分片、主从复制、弹性扩容这些功能都可以做到自动 化，通过简单的部署就可以获得一个大容量、高可靠、高可用的 Redis 集群，并且对于应 用来说，近乎于是透明的。

所以，Redis Cluster 是非常适合构建中小规模 Redis 集群，这里的中小规模指的是，大 概几个到几十个节点这样规模的 Redis 集群。

但是 Redis Cluster 不太适合构建超大规模集群，主要原因是，它采用了去中心化的设 计。刚刚我们讲了，Redis 的每个节点上，都保存了所有槽和节点的映射关系表，客户端可以访问任意一个节点，再通过重定向命令，找到数据所在的那个节点。那你有没有想过一个问题，这个映射关系表，它是如何更新的呢?比如说，集群加入了新节点，或者某个主节点宕机了，新的主节点被选举出来，这些情况下，都需要更新集群每一个节点上的映射关系表。

### 如何用 **Redis** 构建超大规模集群?

一种是基于代理的方式，在客户端和 Redis 节点之间，还需要增加一层代理服务。

这个代理服务有三个作用。

第一个作用是，负责在客户端和 Redis 节点之间转发请求和响应。客户端只和代理服务打交道，代理收到客户端的请求之后，再转发到对应的 Redis 节点上，节点返回的响应再经由代理转发返回给客户端。

第二个作用是，负责监控集群中所有 Redis 节点状态，如果发现有问题节点，及时进行主从切换。

第三个作用就是维护集群的元数据，这个元数据主要就是集群所有节点的主从信息，以及槽和节点关系映射表。

